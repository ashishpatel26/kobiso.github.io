---
title: "Neural machine translation by jointly learning to align and translate"
categories:
  - Research
tags:
  - NLP
  - paper
header:
  teaser: /assets/images/neural machine translation by jointly learning to align and translate/new encoder.png
  overlay_image: /assets/images/neural machine translation by jointly learning to align and translate/new encoder.png
  overlay_filter: 0.4
---

The paper *"Neural Machine Translation By Jointly Learning To Align And Translate"* introduced in 2015 is one of the most famous deep learning paper related natural language process which is cited more than 2,000 times.
This article is a quick summary of the paper.

{% include toc title="Table of Contents" icon="file-text" %}

# Introduction & Background

## Machine Translation (MT)
- **Traditional MT (i.e. Moses)**
  > **Machine translation** is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.
  \- [Wikipedia](https://en.wikipedia.org/wiki/Machine_translation) 

- **Neural MT**
  - Train a large neural network, read a sentence and output a correct translation (i.e. RNN encoder-decoder model).
  - In probabilistic perspective, translation is equivalent to find a target sentence $$y$$ that maximizes the conditional probability of $$y$$ given a source sentence $$x$$ (i.e. $$ argmax_y P(y \mid x) $$).
  - In neural MT, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus.
  - Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.
   
## RNN Encoder - Decoder

![Seq2Seq model example]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/seq2seq.png){: .align-center}

- **RNN Encoder - Decoder model** is previous work of the author on the paper *"Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation"* in 2014.
- RNN Encoder - Decoder model is usually known as a *sequence-to-sequence (seq2seq)* model and it has advantage to understand the sentence correctly since the whole context of the sentence should be considered through context vector $$c$$ that is encoded from input sentence.

![RNN Encoder - Decoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/encoderdecoder.png){: .align-center}

- **Encoder**
  - Reads the input sentences, a sequence of vectors $$x=(x_1, ..., x_{T_x})$$, into a vector $$c$$.
  - Used LSTM model.

$$
h_t = f(x_t, h_{t-1}) \\
c = q(\{h_1, ..., h_{T_x}\})
$${: .text-center}

- **Decoder**
  - Predict the next word $$y_t$$ given the context vector $$c$$.
  - Defines a probability over the translation $$y$$ by decomposing the joint probability into the ordered conditional probabilities.
  - Each conditional probability is modeled by the function $$g$$, which is nonlinear, potentially multi-layered, outputs the probability of $$y_t$$ when $$s_t$$ is a hidden state of the RNN.
  - Used LSTM model.

$$
p(y)=\prod_{t=1}^{T} p(y_t \mid \{y_1, ..., y_{t-1}\}, c) \\
p(y_t \mid \{y_1, ..., y_{t-1}\}, c) = g(y_{t-1}, s_t, c)
$${: .text-center}

- **Problem of RNN Encoder - Decoder model**

![RNN Encoder - Decoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/issue.png){: .align-center}

  - Potential issue is that the network needs to be able to compress all the necessary information of the source sentence into a **fixed-length vector**.
  It is difficult for the neural network to process a long sentences, especially those that are longer than the sentences in the training corpus.
  - Decoder uses **only one context vector $$c$$** to generate output words in all of the RNN steps.
  However, each output world has to be influenced by different input words respectively.
  
# Learning to Align and Translate
## Encoder: Bidirectional RNN for Annotating Sequences

![Proposed encoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/new encoder.png){: .align-center}

- Use **bidirectional RNN** for annotating sequences which consists of forward and backward RNN.
  - This summarize not only the preceding words but also the following words

- **Backward RNN $$\overleftarrow{f}$$**
  - Reads the input sequence in reverse order $$(x_{T_x}, ..., x_1)$$
  - Hidden states: $$(\overleftarrow{h_1}, ..., \overleftarrow{h_{T_s}})$$  

- **Forward RNN $$\overrightarrow{f}$$**
  - Reads the input sequence in proper order $$(x_1, ..., x_{T_x})$$
  - Hidden states: $$(\overrightarrow{h_1}, ..., \overrightarrow{h_{T_x}})$$

- $$(h_1, ..., h_L)$$ is the **new variable-length representation** instead of fixed-length $$c$$.
  - Annotation for each word $$h_j$$: $$h_j = [\overrightarrow{h_{j}^{T}};\overleftarrow{h_{j}^{T}}]^{T}$$
  - $$h_j$$ contains $$x_j$$ together with its context $$(..., x_{j-1}, x_{j+1}, ...)$$.

## Decoder: General Description

![Proposed encoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/new decoder.png){: .align-center}

- Define each conditional probability: $$p(y_i \mid y_1, ..., y_{i-1}, x) = g(y_{i-1}, s_i, c_i)$$
- **$$s_i$$** is an RNN hidden state for time $$i$$: $$s_i = f(s_{i-1}, y_{i-1}, c_i)$$
- **Context $$c_i$$** is computed as a weighted sum of $$h$$:

$$
c_i = \sum_{j=1}^{T_x}\alpha_{ij}h{j} \\
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp_(e_{ik})}
$${: .text-center}

- **Alignment model $$(a)$$** parameterizes it as a feedforward neural network which is jointly trained with all the other components of the system.

$$
e_{ij} = a(s_{i-1}, h_j)
$${: .text-center}

# Experiment and Result

# Conclusion

# References
- Neural Machine Translation by Jointly Learning to Align and Translate [[Link](https://arxiv.org/abs/1409.0473)]
- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation [[Link](https://arxiv.org/abs/1406.1078)]