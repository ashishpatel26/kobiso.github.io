---
title: "Neural machine translation by jointly learning to align and translate"
categories:
  - Research
tags:
  - NLP
  - paper
header:
  teaser: /assets/images/MST.png
  overlay_image: /assets/images/MST.png
  overlay_filter: 0.4
---

The paper *"Neural Machine Translation By Jointly Learning To Align And Translate"* introduced in 2015 is one of the most famous deep learning paper related natural language process which is cited more than 2,000 times.
This article is a quick summary of the paper.

{% include toc title="Table of Contents" icon="file-text" %}

# Introduction & Background

## Machine Translation (MT)
- **Traditional MT (i.e. Moses)**
  > **Machine translation** is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.
  \- [Wikipedia](https://en.wikipedia.org/wiki/Machine_translation) 

- **Neural MT**
  - Train a large neural network, read a sentence and output a correct translation (i.e. RNN encoder-decoder model).
  - In probabilistic perspective, translation is equivalent to find a target sentence $$y$$ that maximizes the conditional probability of $$y$$ given a source sentence $$x$$ (i.e. $$ argmax_y P(y \mid x) $$).
  - In neural MT, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus.
  - Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.
   
## RNN Encoder - Decoder
- **RNN Encoder - Decoder model** is previous work of the author on the paper *"Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation"* in 2014.
- RNN Encoder - Decoder model is usually known as a *sequence-to-sequence (seq2seq)* model and it has advantage to understand the sentence correctly since the whole context of the sentence should be considered through context vector $$c$$ that is encoded from input sentence.

![RNN Encoder - Decoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/encoderdecoder.png){: .align-center}

- **Encoder**
  - Reads the input sentences, a sequence of vectors $$x=(x_1, ..., x_{T_x})$$, into a vector $$c$$.
  - Used LSTM model.

$$
h_t = f(x_t, h_{t-1}) \\
c = q(\{h_1, ..., h_{T_x}\})
$${: .text-center}

- **Decoder**
  - Predict the next word $$y_t$$ given the context vector $$c$$.
  - Defines a probability over the translation $$y$$ by decomposing the joint probability into the ordered conditional probabilities.
  - Each conditional probability is modeled by the function $$g$$, which is nonlinear, potentially multi-layered, outputs the probability of $$y_t$$ when $$s_t$$ is a hidden state of the RNN.
  - Used LSTM model.

$$
p(y)=\prod_{t=1}^{T} p(y_t \mid \{y_1, ..., y_{t-1}\}, c) \\
p(y_t \mid \{y_1, ..., y_{t-1}\}, c) = g(y_{t-1}, s_t, c)
$${: .text-center}

- **Problem of RNN Encoder - Decoder model**

![RNN Encoder - Decoder]({{ site.url }}{{ site.baseurl }}/assets/images/neural machine translation by jointly learning to align and translate/issue.png){: .align-center}

  1. Potential issue is that the network needs to be able to compress all the necessary information of the source sentence into a **fixed-length vector**.
  It is difficult for the neural network to process a long sentences, especially those that are longer than the sentences in the training corpus.
  2. Decoder uses **only one context vector $$c$$** to generate output words in all of the RNN steps.
  However, each output world has to be influenced by different input words respectively.
  
# Learning to Align and Translate

# Experiment and Result

# Conclusion

# References
- Neural Machine Translation by Jointly Learning to Align and Translate [[Link](https://arxiv.org/abs/1409.0473)]
- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation [[Link](https://arxiv.org/abs/1406.1078)]